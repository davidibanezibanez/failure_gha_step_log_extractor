{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c6c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando repositorio: vercel/next.js\n",
      "\n",
      "Procesando run Generate Pull Request Stats (ID: 16040602554)\n",
      "  YAML guardado: pull_request_stats.yml\n",
      "  Job fallido: PR Stats (ID: 45261240385)\n",
      "\n",
      "Procesando run build-and-test (ID: 16039435259)\n",
      "  YAML guardado: build_and_test.yml\n",
      "\n",
      "Procesando run test-e2e-deploy-release (ID: 16038663582)\n",
      "  YAML guardado: test_e2e_deploy_release.yml\n",
      "  Job fallido: test deploy (5/6) / build (ID: 45256145353)\n",
      "  Job fallido: test deploy (2/6) / build (ID: 45256145360)\n",
      "  Job fallido: test deploy (4/6) / build (ID: 45256145361)\n",
      "  Job fallido: test deploy (3/6) / build (ID: 45256145362)\n",
      "  Job fallido: test deploy (1/6) / build (ID: 45256145364)\n",
      "  Job fallido: test deploy (6/6) / build (ID: 45256145365)\n",
      "\n",
      "Procesando run Test examples (ID: 16038559798)\n",
      "  YAML guardado: test_examples.yml\n",
      "  Job fallido: Test Examples (20) (ID: 45255522839)\n",
      "  Job fallido: Test Examples (18) (ID: 45255522848)\n",
      "\n",
      "Procesando run Update Font Data (ID: 16038505226)\n",
      "  YAML guardado: update_fonts_data.yml\n",
      "  Job fallido: create-pull-request (ID: 45255363310)\n",
      "\n",
      "Procesando run build-and-test (ID: 16038458176)\n",
      "  YAML guardado: build_and_test.yml\n",
      "  Job fallido: rust check / build (ID: 45255245708)\n",
      "  Job fallido: test turbopack development integration (5/6) / build (ID: 45255417440)\n",
      "  Job fallido: test turbopack development integration (6/6) / build (ID: 45255417443)\n",
      "\n",
      "Procesando run build-and-test (ID: 16038457870)\n",
      "  YAML guardado: build_and_test.yml\n",
      "\n",
      "Procesando run Lock Threads (ID: 16038393822)\n",
      "  YAML guardado: issue_lock.yml\n",
      "  Job fallido: action (ID: 45255027459)\n",
      "\n",
      "Procesando run build-and-test (ID: 16038273551)\n",
      "  YAML guardado: build_and_test.yml\n",
      "\n",
      "Procesando run Generate Pull Request Stats (ID: 16038164509)\n",
      "  YAML guardado: pull_request_stats.yml\n",
      "  Job fallido: build / build (ID: 45254348827)\n",
      "\n",
      "Análisis completado.\n"
     ]
    }
   ],
   "source": [
    "# Failure GHA step log extractor\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import base64\n",
    "\n",
    "# Configuración GitHub\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "OWNER = \"vercel\"\n",
    "REPO = \"next.js\"\n",
    "\n",
    "def sanitize_filename(name, max_length=100):\n",
    "    name = re.sub(r'[^a-zA-Z0-9 \\-_]', '', name)\n",
    "    return name.strip()[:max_length]\n",
    "\n",
    "# Patrones de inicio de step y detección de error\n",
    "STEP_MARKERS = [\n",
    "    re.compile(r\"##\\[(?:section|group)\\]\\s*(?:Starting:|Run)?\\s*['\\\"]?(.*?)['\\\"]?$\", re.I)\n",
    "]\n",
    "\n",
    "EXIT_CODE_RE = re.compile(r\"(?:exit code|completed with exit code)\\s+(\\d+)\", re.I)\n",
    "\n",
    "def extract_steps(logs: str):\n",
    "    \"\"\"\n",
    "    Divide el log completo de un job en pasos individuales y marca\n",
    "    si cada paso falló o no.\n",
    "\n",
    "    Un paso se considera fallido si:\n",
    "      • Aparece '##[error]' en sus líneas, o\n",
    "      • Contiene 'exit code <n>' / 'completed with exit code <n>'\n",
    "        con n distinto de 0.\n",
    "\n",
    "    El inicio de un step se detecta a través de la existencia de\n",
    "    '##[section]Starting:' y/o '##[group]Run'.\n",
    "    Además se establece que estás dos líneas seguidas no generarán falsas detecciones.\n",
    "    \n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    current_name = None\n",
    "    current_lines = []\n",
    "    current_failed = False\n",
    "    last_step_started_at_line = -10\n",
    "\n",
    "    lines = logs.splitlines()\n",
    "    for idx, line in enumerate(lines):\n",
    "        step_found = False\n",
    "        for pattern in STEP_MARKERS:\n",
    "            match = pattern.search(line)\n",
    "            if match:\n",
    "                name = match.group(1).strip()\n",
    "                # Evita duplicar si viene de inmediato otro marcador\n",
    "                if current_name is not None and (idx - last_step_started_at_line) <= 1:\n",
    "                    continue\n",
    "\n",
    "                # Cierra step anterior\n",
    "                if current_name is not None:\n",
    "                    steps.append({\n",
    "                        \"step_name\": current_name,\n",
    "                        \"logs\": \"\\n\".join(current_lines),\n",
    "                        \"failed\": current_failed,\n",
    "                    })\n",
    "\n",
    "                # Abre nuevo\n",
    "                current_name = name\n",
    "                current_lines = [line]\n",
    "                current_failed = False\n",
    "                last_step_started_at_line = idx\n",
    "                step_found = True\n",
    "                break\n",
    "\n",
    "        if step_found:\n",
    "            continue\n",
    "\n",
    "        current_lines.append(line)\n",
    "\n",
    "        if \"##[error]\" in line:\n",
    "            current_failed = True\n",
    "        else:\n",
    "            m_exit = EXIT_CODE_RE.search(line)\n",
    "            if m_exit and int(m_exit.group(1)) != 0:\n",
    "                current_failed = True\n",
    "\n",
    "    # Cierra último step\n",
    "    if current_name is not None:\n",
    "        steps.append({\n",
    "            \"step_name\": current_name,\n",
    "            \"logs\": \"\\n\".join(current_lines),\n",
    "            \"failed\": current_failed,\n",
    "        })\n",
    "\n",
    "    return steps\n",
    "\n",
    "class GitHubWorkflowLogger:\n",
    "    def __init__(self, token):\n",
    "        self.headers = {\n",
    "            'Authorization': f'token {token}',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        }\n",
    "        self.base_url = 'https://api.github.com'\n",
    "\n",
    "    def get_workflow_runs(self, owner, repo, max_runs=None):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs\"\n",
    "        all_runs = []\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            params = {'status': 'failure', 'per_page': 100, 'page': page}\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, params=params)\n",
    "                response.raise_for_status()\n",
    "                runs = response.json().get('workflow_runs', [])\n",
    "                if not runs:\n",
    "                    break\n",
    "\n",
    "                for run in runs:\n",
    "                    all_runs.append(run)\n",
    "                    if max_runs and len(all_runs) >= max_runs:\n",
    "                        return all_runs\n",
    "\n",
    "                print(f\"Página {page}: {len(runs)} workflows fallidos encontrados.\")\n",
    "                page += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error obteniendo workflow runs (página {page}): {e}\")\n",
    "                break\n",
    "\n",
    "        return all_runs\n",
    "\n",
    "    def get_workflow_run_json(self, owner, repo, run_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs/{run_id}\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error obteniendo detalles del workflow run {run_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_job_logs(self, owner, repo, job_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/jobs/{job_id}/logs\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "\n",
    "    def get_jobs_json(self, owner, repo, run_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs/{run_id}/jobs\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error obteniendo jobs JSON para run {run_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_file_content(self, owner, repo, path, ref=None):\n",
    "        \"\"\"\n",
    "        Devuelve el contenido del archivo en texto plano.\n",
    "        Usa la API /contents que devuelve Base64 si es binario o texto.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/contents/{path}\"\n",
    "        params = {'ref': ref} if ref else {}\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            if data.get(\"encoding\") == \"base64\":\n",
    "                return base64.b64decode(data[\"content\"]).decode(\"utf-8\", errors=\"replace\")\n",
    "            return data.get(\"content\", \"\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error obteniendo contenido de {path} @ {ref}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def create_output_structure(self, owner, repo, run_id, run_name):\n",
    "        base = os.path.join('failure_workflows', f\"{owner}_{repo}\", f\"{run_id}_{sanitize_filename(run_name)}\")\n",
    "        os.makedirs(os.path.join(base, 'jobs'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(base, 'steps'), exist_ok=True)\n",
    "        return base\n",
    "\n",
    "    def save_json(self, data, path):\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def save_text(self, content, path):\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "    # Flujo principal\n",
    "    def analyze_repository(self, owner, repo, max_runs=None):\n",
    "        print(f\"Analizando repositorio: {owner}/{repo}\")\n",
    "        runs = self.get_workflow_runs(owner, repo, max_runs=max_runs)\n",
    "\n",
    "        if not runs:\n",
    "            print(\"No se encontraron workflows fallidos.\")\n",
    "            return\n",
    "\n",
    "        ninety_days_ago = datetime.now(timezone.utc) - timedelta(days=90)\n",
    "\n",
    "        for run in runs:\n",
    "            run_id = run['id']\n",
    "            run_name = run['name']\n",
    "\n",
    "            created_at = datetime.strptime(run['created_at'], \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "            if created_at < ninety_days_ago:\n",
    "                print(f\"  Ignorado (más de 90 días): {run_name} (ID: {run_id})\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcesando run {run_name} (ID: {run_id})\")\n",
    "\n",
    "            run_detail = self.get_workflow_run_json(owner, repo, run_id)\n",
    "            if not run_detail:\n",
    "                continue\n",
    "\n",
    "            run_dir = self.create_output_structure(owner, repo, run_id, run_name)\n",
    "\n",
    "            self.save_json(run_detail, os.path.join(run_dir, 'workflow_run.json'))\n",
    "\n",
    "            jobs_data = self.get_jobs_json(owner, repo, run_id)\n",
    "            if not jobs_data:\n",
    "                continue\n",
    "            self.save_json(jobs_data, os.path.join(run_dir, 'jobs.json'))\n",
    "\n",
    "            # Descargar y guardar YAML del workflow\n",
    "            workflow_path = run_detail.get(\"path\")\n",
    "            head_sha = run_detail.get(\"head_sha\")\n",
    "            if workflow_path:\n",
    "                yaml_content = self.get_file_content(owner, repo, workflow_path, ref=head_sha)\n",
    "                if yaml_content:\n",
    "                    yaml_filename = os.path.basename(workflow_path)\n",
    "                    self.save_text(yaml_content, os.path.join(run_dir, yaml_filename))\n",
    "                    print(f\"  YAML guardado: {yaml_filename}\")\n",
    "\n",
    "            # Procesar cada job fallido (job json, job log, failure steps logs)\n",
    "            for job in jobs_data.get('jobs', []):\n",
    "                if job.get('conclusion') != 'failure':\n",
    "                    continue\n",
    "\n",
    "                job_id = job['id']\n",
    "                job_name = job['name']\n",
    "                print(f\"  Job fallido: {job_name} (ID: {job_id})\")\n",
    "\n",
    "                self.save_json(job, os.path.join(run_dir, 'jobs', f\"{job_id}_{sanitize_filename(job_name)}.json\"))\n",
    "\n",
    "                job_logs = self.get_job_logs(owner, repo, job_id)\n",
    "                if job_logs:\n",
    "                    self.save_text(job_logs, os.path.join(run_dir, 'jobs', f\"{job_id}_{sanitize_filename(job_name)}.txt\"))\n",
    "\n",
    "                    steps = extract_steps(job_logs)\n",
    "                    for step in steps:\n",
    "                        if not step['failed']:\n",
    "                            continue  # solo guardar steps fallidos\n",
    "\n",
    "                        step_filename = f\"{job_id}_{sanitize_filename(step['step_name'])}\"\n",
    "                        step_dir = os.path.join(run_dir, 'steps')\n",
    "\n",
    "                        self.save_text(step['logs'], os.path.join(step_dir, f\"{step_filename}.txt\"))\n",
    "\n",
    "                    time.sleep(1)\n",
    "\n",
    "        print(\"\\nAnálisis completado.\")\n",
    "\n",
    "# Ejecución\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"ERROR: configurar token de GitHub en la variable GITHUB_TOKEN\")\n",
    "elif not OWNER or not REPO:\n",
    "    print(\"ERROR: configurar OWNER y REPO\")\n",
    "else:\n",
    "    logger = GitHubWorkflowLogger(GITHUB_TOKEN)\n",
    "    # logger.analyze_repository(OWNER, REPO) # Sin límite en el campo max_runs\n",
    "    logger.analyze_repository(OWNER, REPO, max_runs=10) # Con límite n en el campo max_runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
