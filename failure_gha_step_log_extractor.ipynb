{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12c6c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analizando repositorio: davidibanezibanez/example-failing-workflows\n",
      "Página 1: 4 workflows fallidos encontrados.\n",
      "\n",
      "Procesando run Fail Workflow 1 (ID: 15814985792)\n",
      "  Job fallido: intentional-fail-job-1 (ID: 44572169286)\n",
      "\n",
      "Procesando run Fail Workflow 4 (ID: 15814985795)\n",
      "  Job fallido: intentional-fail-job-4 (ID: 44572169298)\n",
      "  Job fallido: intentional-fail-job-4_1 (ID: 44572169299)\n",
      "\n",
      "Procesando run Fail Workflow 3 (ID: 15814985796)\n",
      "  Job fallido: intentional-fail-job-3 (ID: 44572169313)\n",
      "\n",
      "Procesando run Fail Workflow 2 (ID: 15814985801)\n",
      "  Job fallido: intentional-fail-job-2 (ID: 44572169303)\n",
      "\n",
      "Análisis completado.\n"
     ]
    }
   ],
   "source": [
    "# Failure GHA step log extractor\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# Configuración\n",
    "load_dotenv()\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "OWNER = \"davidibanezibanez\"\n",
    "REPO = \"example-failing-workflows\"\n",
    "\n",
    "def sanitize_filename(name, max_length=100):\n",
    "    name = re.sub(r'[^a-zA-Z0-9 \\-_]', '', name)\n",
    "    return name.strip()[:max_length]\n",
    "\n",
    "def extract_error_lines(logs):\n",
    "    return '\\n'.join([\n",
    "        line for line in logs.split('\\n')\n",
    "        if '##[error]' in line or 'Error:' in line or 'error:' in line\n",
    "    ]) or '[No se detectó un mensaje de error explícito]'\n",
    "\n",
    "class GitHubWorkflowLogger:\n",
    "    def __init__(self, token):\n",
    "        self.headers = {\n",
    "            'Authorization': f'token {token}',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        }\n",
    "        self.base_url = 'https://api.github.com'\n",
    "\n",
    "    def get_workflow_runs(self, owner, repo, max_runs=None):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs\"\n",
    "        all_runs = []\n",
    "        page = 1\n",
    "\n",
    "        while True:\n",
    "            params = {'status': 'failure', 'per_page': 100, 'page': page}\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, params=params)\n",
    "                response.raise_for_status()\n",
    "                runs = response.json().get('workflow_runs', [])\n",
    "                if not runs:\n",
    "                    break\n",
    "\n",
    "                for run in runs:\n",
    "                    all_runs.append(run)\n",
    "                    if max_runs and len(all_runs) >= max_runs:\n",
    "                        return all_runs\n",
    "\n",
    "                print(f\"Página {page}: {len(runs)} workflows fallidos encontrados.\")\n",
    "                page += 1\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error obteniendo workflow runs (página {page}): {e}\")\n",
    "                break\n",
    "\n",
    "        return all_runs\n",
    "\n",
    "    def get_run_detail_json(self, owner, repo, run_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs/{run_id}\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error obteniendo detalles del workflow run {run_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_job_logs(self, owner, repo, job_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/jobs/{job_id}/logs\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException:\n",
    "            return None\n",
    "\n",
    "    def get_jobs_json(self, owner, repo, run_id):\n",
    "        url = f\"{self.base_url}/repos/{owner}/{repo}/actions/runs/{run_id}/jobs\"\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error obteniendo jobs JSON para run {run_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_steps(self, logs):\n",
    "        steps = []\n",
    "        lines = logs.split('\\n')\n",
    "        current_step = None\n",
    "        step_logs = []\n",
    "\n",
    "        for line in lines:\n",
    "            if '##[group]' in line and 'Run ' in line:\n",
    "                if current_step:\n",
    "                    steps.append({\n",
    "                        'step_name': current_step,\n",
    "                        'logs': '\\n'.join(step_logs),\n",
    "                        'failed': any('##[error]' in l or 'Error:' in l or 'error:' in l for l in step_logs)\n",
    "                    })\n",
    "                current_step = line.replace('##[group]', '').strip()\n",
    "                step_logs = []\n",
    "            step_logs.append(line)\n",
    "\n",
    "        if current_step:\n",
    "            steps.append({\n",
    "                'step_name': current_step,\n",
    "                'logs': '\\n'.join(step_logs),\n",
    "                'failed': any('##[error]' in l or 'Error:' in l or 'error:' in l for l in step_logs)\n",
    "            })\n",
    "        return steps\n",
    "\n",
    "    def create_output_structure(self, owner, repo, run_id, run_name):\n",
    "        base = os.path.join('github_failure_logs', f\"{owner}_{repo}\", f\"{run_id}_{sanitize_filename(run_name)}\")\n",
    "        os.makedirs(os.path.join(base, 'jobs'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(base, 'steps'), exist_ok=True)\n",
    "        return base\n",
    "\n",
    "    def save_json(self, data, path):\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "\n",
    "    def save_text(self, content, path):\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "\n",
    "    def analyze_repository(self, owner, repo, max_runs=None):\n",
    "        print(f\"Analizando repositorio: {owner}/{repo}\")\n",
    "        runs = self.get_workflow_runs(owner, repo, max_runs=max_runs)\n",
    "\n",
    "        if not runs:\n",
    "            print(\"No se encontraron workflows fallidos.\")\n",
    "            return\n",
    "\n",
    "        ninety_days_ago = datetime.now(timezone.utc) - timedelta(days=90)\n",
    "\n",
    "        for run in runs:\n",
    "            run_id = run['id']\n",
    "            run_name = run['name']\n",
    "\n",
    "            created_at = datetime.strptime(run['created_at'], \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "            if created_at < ninety_days_ago:\n",
    "                print(f\"  Ignorado (más de 90 días): {run_name} (ID: {run_id})\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\nProcesando run {run_name} (ID: {run_id})\")\n",
    "\n",
    "            run_detail = self.get_run_detail_json(owner, repo, run_id)\n",
    "            if not run_detail:\n",
    "                continue\n",
    "\n",
    "            run_dir = self.create_output_structure(owner, repo, run_id, run_name)\n",
    "\n",
    "            self.save_json(run_detail, os.path.join(run_dir, 'workflow_run.json'))\n",
    "\n",
    "            jobs_data = self.get_jobs_json(owner, repo, run_id)\n",
    "            if not jobs_data:\n",
    "                continue\n",
    "\n",
    "            self.save_json(jobs_data, os.path.join(run_dir, 'jobs.json'))\n",
    "\n",
    "            for job in jobs_data.get('jobs', []):\n",
    "                if job.get('conclusion') != 'failure':\n",
    "                    continue\n",
    "\n",
    "                job_id = job['id']\n",
    "                job_name = job['name']\n",
    "                print(f\"  Job fallido: {job_name} (ID: {job_id})\")\n",
    "\n",
    "                self.save_json(job, os.path.join(run_dir, 'jobs', f\"{job_id}_{sanitize_filename(job_name)}.json\"))\n",
    "\n",
    "                job_logs = self.get_job_logs(owner, repo, job_id)\n",
    "                if job_logs:\n",
    "                    self.save_text(job_logs, os.path.join(run_dir, 'jobs', f\"{job_id}_{sanitize_filename(job_name)}.txt\"))\n",
    "\n",
    "                    steps = self.extract_steps(job_logs)\n",
    "                    for step in steps:\n",
    "                        prefix = \"FAILED_\" if step['failed'] else \"OK_\"\n",
    "                        step_filename = f\"{job_id}_{prefix}{sanitize_filename(step['step_name'])}\"\n",
    "                        step_dir = os.path.join(run_dir, 'steps')\n",
    "\n",
    "                        self.save_text(step['logs'], os.path.join(step_dir, f\"{step_filename}.txt\"))\n",
    "\n",
    "                        if step['failed']:\n",
    "                            self.save_text(\n",
    "                                extract_error_lines(step['logs']),\n",
    "                                os.path.join(step_dir, f\"{step_filename}_errors.txt\")\n",
    "                            )\n",
    "\n",
    "            time.sleep(0.5)\n",
    "\n",
    "        print(\"\\nAnálisis completado.\")\n",
    "\n",
    "# Ejecución\n",
    "if not GITHUB_TOKEN:\n",
    "    print(\"ERROR: configurar token de GitHub en la variable GITHUB_TOKEN\")\n",
    "elif not OWNER or not REPO:\n",
    "    print(\"ERROR: configurar OWNER y REPO\")\n",
    "else:\n",
    "    logger = GitHubWorkflowLogger(GITHUB_TOKEN)\n",
    "    logger.analyze_repository(OWNER, REPO) # Sin máximo de workflow runs\n",
    "    #logger.analyze_repository(OWNER, REPO, max_runs=10) # Máximo de n workflow runs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
